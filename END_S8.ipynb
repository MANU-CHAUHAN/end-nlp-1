{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXHS3OPeK2X9"
      },
      "source": [
        "# *Major take away from Seq 2 Seq paper:*\n",
        "1) Deeper LSTMs work better than shallow ones, hence authors used 4 layered LSTM\n",
        "\n",
        "2) Use of separate LSTMs as well embeddings for encoder and decoder blocks\n",
        "\n",
        "3) For Neural Machine Translation, reversing the order of sentence in source dataset for training produces short term dependencies, as the beginning of source sentence in read towards end, resulting in better overall score on test benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD04MHzSzqUV"
      },
      "source": [
        "# !pip install --upgrade torch\n",
        "# !pip install -U torchtext==0.8.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEv73ZBOp9bO"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsBVKN5mH5Zc"
      },
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzekLUtXL50z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b6e8db-377b-46d1-b0f1-ada42250ae4d"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de\n",
        "!python -m spacy download fr"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907057 sha256=f5f31daa2b1053992f7db5b6baaf51969729e0a198318b24bb00d104041e4d80\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hnpiu3i_/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp37-none-any.whl size=14727027 sha256=8edd4ba41a571850883971a4a658d97f40cfdd628b314c3fd0587978f4b34d9a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-on671h4s/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFEkVvvKNQa1"
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrOvT9RUNek0"
      },
      "source": [
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1] # reverse as per the enhanced performance observation in original Seq2Seq paper\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r6ZTGgQN68s"
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en,\n",
        "            init_token= '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower=True)\n",
        "\n",
        "TRG = Field(tokenize=tokenize_de,\n",
        "            init_token='<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C2spisVPgHO"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.en', '.de'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGztAdvZQO7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf87db4-c0ca-44e1-89cc-d7d4a49e08a6"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7q_2zzDRz_0"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17FpoN_GSrtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01713756-f846-454f-aedb-7dc122cff415"
      },
      "source": [
        "print(f\"Unique tokens in source (en) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (de) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (en) vocabulary: 5893\n",
            "Unique tokens in target (de) vocabulary: 7855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpxYqBHRSt9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624d95eb-e82d-4453-b443-c02464214dde"
      },
      "source": [
        "vars(train_data[0])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'src': ['two',\n",
              "  'young',\n",
              "  ',',\n",
              "  'white',\n",
              "  'males',\n",
              "  'are',\n",
              "  'outside',\n",
              "  'near',\n",
              "  'many',\n",
              "  'bushes',\n",
              "  '.'],\n",
              " 'trg': ['.',\n",
              "  'büsche',\n",
              "  'vieler',\n",
              "  'nähe',\n",
              "  'der',\n",
              "  'in',\n",
              "  'freien',\n",
              "  'im',\n",
              "  'sind',\n",
              "  'männer',\n",
              "  'weiße',\n",
              "  'junge',\n",
              "  'zwei']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPUig3_RS2AF"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_3z1Ht5z-PE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2b2ab8-3393-4242-ba3e-0260bdd65ac3"
      },
      "source": [
        "device"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdEM710dbq5A"
      },
      "source": [
        "batch=128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                      batch_size=batch,\n",
        "                                                                      device = device)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY8LlJHeb_BY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        return hidden, cell\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ZNQHHopeL1"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, emb_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim , n_layers, dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAJ-YAJhW6qY"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.n_layers == decoder.n_layers, \"Please make sure that the number of layers in LSTM for both Encoder and Decoder are same\"\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"Please make sure that the hidden layer size in LSTM for both Encoder and Decoder are same\"\n",
        "    \n",
        "    def forward(self, source, target, teacher_forcing=0.5):\n",
        "        # src = [source length, batch_size],  target = [target length, batch_size]\n",
        "        trg_len, batch_size = target.shape[0], target.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # decoder output tensor, shape = [target_sentence_length, batch_size, target vocab size],\n",
        "        # each sample has different len where each token has same vector size(vocab size)\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # last hidden and cell states from the Encoder section are used as the\n",
        "        # initial hidden inputs for the first iteration of Decoder time step\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        input_ = target[0, :] # the '<sos>' tokens as input for the decoder first time step\n",
        "\n",
        "        for t in range(1, trg_len): # 1 to trg len as first token will be <sos>\n",
        "            output_pred, hidden, cell = self.decoder(input_, hidden, cell) # input embedding, hidden and cell states -> prediction, hidden and cell as outputs\n",
        "\n",
        "            outputs[t] = output_pred\n",
        "\n",
        "            teacher = random.random() < teacher_forcing\n",
        "\n",
        "            top_1 = output_pred.argmax(1) # get prediction with highest value\n",
        "\n",
        "            # decide if to use predicted output as input to next time step or ground truth token\n",
        "            input_ = target[t] if teacher else top_1\n",
        "        return outputs\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdZx1I-_swLr"
      },
      "source": [
        "## Training Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbMVytOisG8y"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "Enc_emd_dim = Dec_emd_dim =256\n",
        "\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2 # paper uses 4 though \n",
        "enc_dropout = dec_dropout = 0.5\n",
        "\n",
        "enc = Encoder(input_dim=INPUT_DIM, emb_dim=Enc_emd_dim, hid_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout=enc_dropout)\n",
        "dec = Decoder(output_dim=OUTPUT_DIM, emb_dim=Dec_emd_dim, hid_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout=dec_dropout)\n",
        "model = Seq2Seq(encoder=enc, decoder=dec, device=device).to(device) "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbfxUcbawI9V"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df0j9paO05dI"
      },
      "source": [
        "## Initialize weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK7oZGVp0GQc"
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bjG46lL1L-a"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXGzbZWBznjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aee4694-e775-4246-aeda-6e552b2b6c9f"
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 14,905,519 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW5kDlpwzn_a"
      },
      "source": [
        "# intialize an optimier for training\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3DIxsH31ECY"
      },
      "source": [
        "### # as we can have <pad> tpoken as well in src and trg sentences and we should not calculate loss on it, hence we ignore <pad> topken while calculating loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "matN-hvW0XV6"
      },
      "source": [
        "trg_pad_index = TRG.vocab.stoi[TRG.pad_token]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXTG4Kn21Mzk"
      },
      "source": [
        "## we define crossentropy loss criterion for calculating loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmIeZVuk1MDM"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_index)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4Q7o3F8nhF"
      },
      "source": [
        "##Training process:\n",
        "\n",
        "1) get `source` and `target` sentences\n",
        "\n",
        "2) ensure to make gradients to zero to avoid accumulation of gradients over time\n",
        "\n",
        "3) get the output `y_hat` from model after feeding in `source` and `target`\n",
        "\n",
        "4) calculate loss after changing dimensions of the input and output correctly, use `loss.backward()` to calculate gradients\n",
        "\n",
        "5) clip the gradients to avoid `graident explosion`, a common issue with RNNs\n",
        "\n",
        "6) update the model weights\n",
        "\n",
        "7) calculate total loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cExAoFuA1UJH"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \"\"\"Train function to train the `model` with given `optimizer` using `criterion`\"\"\"\n",
        "    model.train() # set model to train mode\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        source, target = batch.src, batch.trg # get source and target batch\n",
        "\n",
        "        # zero out previous gradients if any\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get output from model\n",
        "        output = model(source, target)\n",
        "\n",
        "        # get output dimension, target = [trg len , batch size], output = [trg len, batch size, target vocab/output dim]\n",
        "        output_dim = output.shape[-1] # get target vocab size from last dimension\n",
        "\n",
        "        # reshape output for usage in loss calculation\n",
        "        # this line results in output = [target len * batch size, target vocab size]\n",
        "        output = output[1:].view(-1, output_dim) # ignore `0` index values as it holds all zeros\n",
        "        \n",
        "        # change view of target to target_len*batch_size to make it 1-d vector, \n",
        "        # while ignoreing first token of each sample as it holds <sos> token\n",
        "        target = target[1:].view(-1) \n",
        "\n",
        "        loss = criterion(output, target) # calculate loss\n",
        "\n",
        "        loss.backward() # calculate gradients on the loss\n",
        "\n",
        "        # clip to gradients to avoid gradient explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # take a single optimization step for the optimizer\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() # accumulate loss per epoch\n",
        "\n",
        "    return total_loss / len(iterator) # return avergae loss\n",
        "         \n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-IjnX7ZEKHD"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval() # set model to evaluate mode\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad(): # since we do not need to calculate graidents in evaluation mode\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg, 0) #since we are evaluating hence turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "            output_dim = output.shape[-1] # get target vocab size from last dimension\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim) # view for target len * batch size, output_dim\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFSMiQcFGr_7"
      },
      "source": [
        "def epoch_time(start, end):\n",
        "    total_time=end-start\n",
        "    mins = total_time//60\n",
        "    seconds = total_time % 60\n",
        "    return mins, seconds"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPsF1pf1G_ol"
      },
      "source": [
        "## make sure to save best model after check at every epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRYjooXvG39O"
      },
      "source": [
        "epochs = 20\n",
        "clip = 1\n",
        "\n",
        "best_valid_loss = float('-inf')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do8wKFAUHKRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed7a0de-ba95-4738-e254-296acbeaa13f"
      },
      "source": [
        "print(\"\\n\\n\")\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
        "\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    total_mins, total_secs = epoch_time(start=start_time, end=end_time)\n",
        "\n",
        "    print(f'\\nEpoch:---------------->{epoch+1:02} | Time: {total_mins}m and {round(total_secs, 2)}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:---------------->01 | Time: 0.0m and 42.48s\n",
            "\tTrain Loss: 5.187 | Train PPL: 178.910\n",
            "\t Val. Loss: 5.241 |  Val. PPL: 188.790\n",
            "\n",
            "Epoch:---------------->02 | Time: 0.0m and 43.32s\n",
            "\tTrain Loss: 4.641 | Train PPL: 103.618\n",
            "\t Val. Loss: 4.963 |  Val. PPL: 143.064\n",
            "\n",
            "Epoch:---------------->03 | Time: 0.0m and 43.42s\n",
            "\tTrain Loss: 4.370 | Train PPL:  79.030\n",
            "\t Val. Loss: 4.886 |  Val. PPL: 132.438\n",
            "\n",
            "Epoch:---------------->04 | Time: 0.0m and 43.34s\n",
            "\tTrain Loss: 4.117 | Train PPL:  61.395\n",
            "\t Val. Loss: 4.741 |  Val. PPL: 114.565\n",
            "\n",
            "Epoch:---------------->05 | Time: 0.0m and 43.4s\n",
            "\tTrain Loss: 3.884 | Train PPL:  48.611\n",
            "\t Val. Loss: 4.624 |  Val. PPL: 101.945\n",
            "\n",
            "Epoch:---------------->06 | Time: 0.0m and 43.71s\n",
            "\tTrain Loss: 3.711 | Train PPL:  40.915\n",
            "\t Val. Loss: 4.441 |  Val. PPL:  84.868\n",
            "\n",
            "Epoch:---------------->07 | Time: 0.0m and 43.27s\n",
            "\tTrain Loss: 3.578 | Train PPL:  35.800\n",
            "\t Val. Loss: 4.369 |  Val. PPL:  78.994\n",
            "\n",
            "Epoch:---------------->08 | Time: 0.0m and 43.33s\n",
            "\tTrain Loss: 3.410 | Train PPL:  30.256\n",
            "\t Val. Loss: 4.334 |  Val. PPL:  76.257\n",
            "\n",
            "Epoch:---------------->09 | Time: 0.0m and 43.3s\n",
            "\tTrain Loss: 3.296 | Train PPL:  27.017\n",
            "\t Val. Loss: 4.294 |  Val. PPL:  73.230\n",
            "\n",
            "Epoch:---------------->10 | Time: 0.0m and 43.12s\n",
            "\tTrain Loss: 3.187 | Train PPL:  24.210\n",
            "\t Val. Loss: 4.194 |  Val. PPL:  66.310\n",
            "\n",
            "Epoch:---------------->11 | Time: 0.0m and 43.54s\n",
            "\tTrain Loss: 3.080 | Train PPL:  21.749\n",
            "\t Val. Loss: 4.181 |  Val. PPL:  65.426\n",
            "\n",
            "Epoch:---------------->12 | Time: 0.0m and 43.49s\n",
            "\tTrain Loss: 2.963 | Train PPL:  19.363\n",
            "\t Val. Loss: 4.183 |  Val. PPL:  65.590\n",
            "\n",
            "Epoch:---------------->13 | Time: 0.0m and 43.05s\n",
            "\tTrain Loss: 2.851 | Train PPL:  17.306\n",
            "\t Val. Loss: 4.052 |  Val. PPL:  57.523\n",
            "\n",
            "Epoch:---------------->14 | Time: 0.0m and 43.28s\n",
            "\tTrain Loss: 2.775 | Train PPL:  16.040\n",
            "\t Val. Loss: 4.072 |  Val. PPL:  58.664\n",
            "\n",
            "Epoch:---------------->15 | Time: 0.0m and 43.26s\n",
            "\tTrain Loss: 2.693 | Train PPL:  14.774\n",
            "\t Val. Loss: 4.053 |  Val. PPL:  57.575\n",
            "\n",
            "Epoch:---------------->16 | Time: 0.0m and 43.3s\n",
            "\tTrain Loss: 2.594 | Train PPL:  13.384\n",
            "\t Val. Loss: 4.048 |  Val. PPL:  57.304\n",
            "\n",
            "Epoch:---------------->17 | Time: 0.0m and 43.41s\n",
            "\tTrain Loss: 2.514 | Train PPL:  12.351\n",
            "\t Val. Loss: 3.944 |  Val. PPL:  51.614\n",
            "\n",
            "Epoch:---------------->18 | Time: 0.0m and 43.24s\n",
            "\tTrain Loss: 2.404 | Train PPL:  11.064\n",
            "\t Val. Loss: 3.986 |  Val. PPL:  53.848\n",
            "\n",
            "Epoch:---------------->19 | Time: 0.0m and 43.37s\n",
            "\tTrain Loss: 2.331 | Train PPL:  10.286\n",
            "\t Val. Loss: 3.963 |  Val. PPL:  52.626\n",
            "\n",
            "Epoch:---------------->20 | Time: 0.0m and 43.27s\n",
            "\tTrain Loss: 2.266 | Train PPL:   9.642\n",
            "\t Val. Loss: 4.014 |  Val. PPL:  55.386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bIPig3DQQSV"
      },
      "source": [
        "# Different hyper-parameters -> Experiment number 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHe-d10tQUTy"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "Enc_emd_dim = Dec_emd_dim = 512 # previous one had 256\n",
        "\n",
        "HIDDEN_DIM = 1024 # previously 512\n",
        "N_LAYERS = 4 # previously 2\n",
        "enc_dropout = dec_dropout = 0.5\n",
        "\n",
        "enc = Encoder(input_dim=INPUT_DIM, emb_dim=Enc_emd_dim, hid_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout=enc_dropout)\n",
        "dec = Decoder(output_dim=OUTPUT_DIM, emb_dim=Dec_emd_dim, hid_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout=dec_dropout)\n",
        "model_2 = Seq2Seq(encoder=enc, decoder=dec, device=device).to(device) "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHpxqo5QQUT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce91442e-0781-40f0-a677-a51e3178ae45"
      },
      "source": [
        "model_2.apply(init_weights)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5893, 512)\n",
              "    (rnn): LSTM(512, 1024, num_layers=4, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(7855, 512)\n",
              "    (rnn): LSTM(512, 1024, num_layers=4, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=7855, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjtArln2RR8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56012d32-e3d4-4328-a99d-8c0e2750f8a8"
      },
      "source": [
        "print(f' model_2({model_2})\\n has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " model_2(Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(5893, 512)\n",
            "    (rnn): LSTM(512, 1024, num_layers=4, dropout=0.5)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(7855, 512)\n",
            "    (rnn): LSTM(512, 1024, num_layers=4, dropout=0.5)\n",
            "    (fc): Linear(in_features=1024, out_features=7855, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "))\n",
            " has 14,905,519 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iy44Kf9Q9iY"
      },
      "source": [
        "epochs = 15\n",
        "clip = 1\n",
        "optimizer = optim.Adam(model_2.parameters())\n",
        "best_valid_loss = float('-inf')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rjExZQzQ-eT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7283ac59-5756-4c48-96e3-7913b67eaf97"
      },
      "source": [
        "print(\"\\n\\n\\nExperiment number:2 \\n\\n\")\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    train_loss = train(model_2, train_iterator, optimizer, criterion, clip)\n",
        "\n",
        "    valid_loss = evaluate(model_2, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    total_mins, total_secs = epoch_time(start=start_time, end=end_time)\n",
        "\n",
        "    print(f'\\nEpoch:---------------->{epoch+1:02} | Time: {total_mins}m and {round(total_secs, 2)}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Experiment number:2 \n",
            "\n",
            "\n",
            "\n",
            "Epoch:---------------->01 | Time: 2.0m and 29.76s\n",
            "\tTrain Loss: 5.187 | Train PPL: 178.864\n",
            "\t Val. Loss: 4.874 |  Val. PPL: 130.902\n",
            "\n",
            "Epoch:---------------->02 | Time: 2.0m and 30.55s\n",
            "\tTrain Loss: 4.830 | Train PPL: 125.177\n",
            "\t Val. Loss: 4.824 |  Val. PPL: 124.518\n",
            "\n",
            "Epoch:---------------->03 | Time: 2.0m and 30.87s\n",
            "\tTrain Loss: 4.367 | Train PPL:  78.788\n",
            "\t Val. Loss: 4.710 |  Val. PPL: 111.002\n",
            "\n",
            "Epoch:---------------->04 | Time: 2.0m and 30.15s\n",
            "\tTrain Loss: 4.071 | Train PPL:  58.629\n",
            "\t Val. Loss: 4.694 |  Val. PPL: 109.333\n",
            "\n",
            "Epoch:---------------->05 | Time: 2.0m and 30.91s\n",
            "\tTrain Loss: 3.831 | Train PPL:  46.126\n",
            "\t Val. Loss: 4.447 |  Val. PPL:  85.367\n",
            "\n",
            "Epoch:---------------->06 | Time: 2.0m and 29.91s\n",
            "\tTrain Loss: 3.642 | Train PPL:  38.176\n",
            "\t Val. Loss: 4.345 |  Val. PPL:  77.116\n",
            "\n",
            "Epoch:---------------->07 | Time: 2.0m and 31.32s\n",
            "\tTrain Loss: 3.479 | Train PPL:  32.416\n",
            "\t Val. Loss: 4.388 |  Val. PPL:  80.509\n",
            "\n",
            "Epoch:---------------->08 | Time: 2.0m and 31.12s\n",
            "\tTrain Loss: 3.313 | Train PPL:  27.479\n",
            "\t Val. Loss: 4.356 |  Val. PPL:  77.945\n",
            "\n",
            "Epoch:---------------->09 | Time: 2.0m and 30.54s\n",
            "\tTrain Loss: 3.168 | Train PPL:  23.749\n",
            "\t Val. Loss: 4.394 |  Val. PPL:  80.931\n",
            "\n",
            "Epoch:---------------->10 | Time: 2.0m and 30.63s\n",
            "\tTrain Loss: 3.070 | Train PPL:  21.539\n",
            "\t Val. Loss: 4.320 |  Val. PPL:  75.211\n",
            "\n",
            "Epoch:---------------->11 | Time: 2.0m and 31.49s\n",
            "\tTrain Loss: 2.927 | Train PPL:  18.663\n",
            "\t Val. Loss: 4.278 |  Val. PPL:  72.065\n",
            "\n",
            "Epoch:---------------->12 | Time: 2.0m and 31.01s\n",
            "\tTrain Loss: 2.832 | Train PPL:  16.985\n",
            "\t Val. Loss: 4.296 |  Val. PPL:  73.379\n",
            "\n",
            "Epoch:---------------->13 | Time: 2.0m and 30.19s\n",
            "\tTrain Loss: 2.696 | Train PPL:  14.819\n",
            "\t Val. Loss: 4.388 |  Val. PPL:  80.466\n",
            "\n",
            "Epoch:---------------->14 | Time: 2.0m and 30.18s\n",
            "\tTrain Loss: 2.589 | Train PPL:  13.318\n",
            "\t Val. Loss: 4.326 |  Val. PPL:  75.651\n",
            "\n",
            "Epoch:---------------->15 | Time: 2.0m and 30.8s\n",
            "\tTrain Loss: 2.513 | Train PPL:  12.337\n",
            "\t Val. Loss: 4.378 |  Val. PPL:  79.648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dvf8eWjDQZh"
      },
      "source": [
        "## Clearly the experiment requires longer training to achieve better scores. More data would also be helpful as hidden and embeddings are much bigger in size than experiment number 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQGTdmb3Rjjf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE_YpbzfSdCX"
      },
      "source": [
        "# Different hyper-parameters -> Experiment number 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xevx7oCRSdCa"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "\n",
        "Enc_emd_dim = Dec_emd_dim = 256 # previous one had 512\n",
        "\n",
        "HIDDEN_DIM = 512 # previously 1024\n",
        "N_LAYERS = 4\n",
        "enc_dropout = dec_dropout = 0.3\n",
        "\n",
        "enc = Encoder(input_dim=INPUT_DIM, emb_dim=Enc_emd_dim, hid_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout=enc_dropout)\n",
        "dec = Decoder(output_dim=OUTPUT_DIM, emb_dim=Dec_emd_dim, hid_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout=dec_dropout)\n",
        "model_3 = Seq2Seq(encoder=enc, decoder=dec, device=device).to(device) \n",
        "\n",
        "optimizer = optim.Adam(model_3.parameters())"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4eyyY_YSdCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bae592-a816-4196-da36-55f8573631cd"
      },
      "source": [
        "model_3.apply(init_weights)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=4, dropout=0.3)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=4, dropout=0.3)\n",
              "    (fc): Linear(in_features=512, out_features=7855, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojamsd1NSdCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c32d14-817e-4786-cc71-4dde4c1e4b75"
      },
      "source": [
        "print(f' model_3({model_3})\\n has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " model_3(Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(5893, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=4, dropout=0.3)\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(7855, 256)\n",
            "    (rnn): LSTM(256, 512, num_layers=4, dropout=0.3)\n",
            "    (fc): Linear(in_features=512, out_features=7855, bias=True)\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "))\n",
            " has 14,905,519 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHou1gVSSdCc"
      },
      "source": [
        "epochs = 20\n",
        "clip = 1\n",
        "\n",
        "best_valid_loss = float('-inf')"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzsY4vloSdCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dcef19c-bd1e-49f8-815d-5cc04709f863"
      },
      "source": [
        "print(\"\\n\\n\\nExperiment number:3 \\n\\n\")\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    train_loss = train(model_3, train_iterator, optimizer, criterion, clip)\n",
        "\n",
        "    valid_loss = evaluate(model_3, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    total_mins, total_secs = epoch_time(start=start_time, end=end_time)\n",
        "\n",
        "    print(f'\\nEpoch:---------------->{epoch+1:02} | Time: {total_mins}m and {round(total_secs, 2)}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Experiment number:3 \n",
            "\n",
            "\n",
            "\n",
            "Epoch:---------------->01 | Time: 1.0m and 0.09s\n",
            "\tTrain Loss: 5.202 | Train PPL: 181.665\n",
            "\t Val. Loss: 5.021 |  Val. PPL: 151.634\n",
            "\n",
            "Epoch:---------------->02 | Time: 1.0m and 0.78s\n",
            "\tTrain Loss: 4.790 | Train PPL: 120.268\n",
            "\t Val. Loss: 4.925 |  Val. PPL: 137.736\n",
            "\n",
            "Epoch:---------------->03 | Time: 1.0m and 1.42s\n",
            "\tTrain Loss: 4.420 | Train PPL:  83.132\n",
            "\t Val. Loss: 4.735 |  Val. PPL: 113.891\n",
            "\n",
            "Epoch:---------------->04 | Time: 1.0m and 1.86s\n",
            "\tTrain Loss: 4.131 | Train PPL:  62.210\n",
            "\t Val. Loss: 4.638 |  Val. PPL: 103.344\n",
            "\n",
            "Epoch:---------------->05 | Time: 1.0m and 1.99s\n",
            "\tTrain Loss: 3.947 | Train PPL:  51.786\n",
            "\t Val. Loss: 4.596 |  Val. PPL:  99.081\n",
            "\n",
            "Epoch:---------------->06 | Time: 1.0m and 2.08s\n",
            "\tTrain Loss: 3.784 | Train PPL:  43.991\n",
            "\t Val. Loss: 4.582 |  Val. PPL:  97.756\n",
            "\n",
            "Epoch:---------------->07 | Time: 1.0m and 2.4s\n",
            "\tTrain Loss: 3.621 | Train PPL:  37.360\n",
            "\t Val. Loss: 4.538 |  Val. PPL:  93.488\n",
            "\n",
            "Epoch:---------------->08 | Time: 1.0m and 2.14s\n",
            "\tTrain Loss: 3.502 | Train PPL:  33.179\n",
            "\t Val. Loss: 4.423 |  Val. PPL:  83.361\n",
            "\n",
            "Epoch:---------------->09 | Time: 1.0m and 2.0s\n",
            "\tTrain Loss: 3.393 | Train PPL:  29.748\n",
            "\t Val. Loss: 4.335 |  Val. PPL:  76.360\n",
            "\n",
            "Epoch:---------------->10 | Time: 1.0m and 1.98s\n",
            "\tTrain Loss: 3.263 | Train PPL:  26.121\n",
            "\t Val. Loss: 4.295 |  Val. PPL:  73.327\n",
            "\n",
            "Epoch:---------------->11 | Time: 1.0m and 2.21s\n",
            "\tTrain Loss: 3.170 | Train PPL:  23.815\n",
            "\t Val. Loss: 4.269 |  Val. PPL:  71.477\n",
            "\n",
            "Epoch:---------------->12 | Time: 1.0m and 2.18s\n",
            "\tTrain Loss: 3.071 | Train PPL:  21.565\n",
            "\t Val. Loss: 4.208 |  Val. PPL:  67.223\n",
            "\n",
            "Epoch:---------------->13 | Time: 1.0m and 2.29s\n",
            "\tTrain Loss: 2.983 | Train PPL:  19.753\n",
            "\t Val. Loss: 4.195 |  Val. PPL:  66.345\n",
            "\n",
            "Epoch:---------------->14 | Time: 1.0m and 1.99s\n",
            "\tTrain Loss: 2.869 | Train PPL:  17.625\n",
            "\t Val. Loss: 4.163 |  Val. PPL:  64.267\n",
            "\n",
            "Epoch:---------------->15 | Time: 1.0m and 1.86s\n",
            "\tTrain Loss: 2.777 | Train PPL:  16.064\n",
            "\t Val. Loss: 4.121 |  Val. PPL:  61.627\n",
            "\n",
            "Epoch:---------------->16 | Time: 1.0m and 1.91s\n",
            "\tTrain Loss: 2.686 | Train PPL:  14.676\n",
            "\t Val. Loss: 4.210 |  Val. PPL:  67.334\n",
            "\n",
            "Epoch:---------------->17 | Time: 1.0m and 2.1s\n",
            "\tTrain Loss: 2.607 | Train PPL:  13.555\n",
            "\t Val. Loss: 4.189 |  Val. PPL:  65.976\n",
            "\n",
            "Epoch:---------------->18 | Time: 1.0m and 2.03s\n",
            "\tTrain Loss: 2.524 | Train PPL:  12.478\n",
            "\t Val. Loss: 4.058 |  Val. PPL:  57.876\n",
            "\n",
            "Epoch:---------------->19 | Time: 1.0m and 2.19s\n",
            "\tTrain Loss: 2.492 | Train PPL:  12.080\n",
            "\t Val. Loss: 4.125 |  Val. PPL:  61.853\n",
            "\n",
            "Epoch:---------------->20 | Time: 1.0m and 1.79s\n",
            "\tTrain Loss: 2.411 | Train PPL:  11.147\n",
            "\t Val. Loss: 4.048 |  Val. PPL:  57.261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvKvGdB4St2T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}